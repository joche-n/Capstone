{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced30fe8",
   "metadata": {},
   "source": [
    "xxx yolos\n",
    "\n",
    "what do we need for yolo?\n",
    "get all the labeld images?\n",
    "\n",
    "transfrome labels?\n",
    "\n",
    "weights?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a55852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection,AutoImageProcessor\n",
    "from PIL import Image, ImageDraw\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4453d288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-base\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4028161d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported number of image dimensions: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m img = Image.open(\u001b[33m\"\u001b[39m\u001b[33mtomo_00e463/slice_0235.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_img=\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Spiced/Capstone/kaggle_cryo/.venv/lib/python3.11/site-packages/transformers/image_processing_utils.py:42\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Spiced/Capstone/kaggle_cryo/.venv/lib/python3.11/site-packages/transformers/models/yolos/image_processing_yolos.py:1353\u001b[39m, in \u001b[36mYolosImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_annotations, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)\u001b[39m\n\u001b[32m   1346\u001b[39m     logger.warning_once(\n\u001b[32m   1347\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1349\u001b[39m     )\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1352\u001b[39m     \u001b[38;5;66;03m# We assume that all images have the same channel dimension format.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m     input_data_format = \u001b[43minfer_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[38;5;66;03m# prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\u001b[39;00m\n\u001b[32m   1356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Spiced/Capstone/kaggle_cryo/.venv/lib/python3.11/site-packages/transformers/image_utils.py:364\u001b[39m, in \u001b[36minfer_channel_dimension_format\u001b[39m\u001b[34m(image, num_channels)\u001b[39m\n\u001b[32m    362\u001b[39m     first_dim, last_dim = \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported number of image dimensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image.shape[first_dim] \u001b[38;5;129;01min\u001b[39;00m num_channels \u001b[38;5;129;01mand\u001b[39;00m image.shape[last_dim] \u001b[38;5;129;01min\u001b[39;00m num_channels:\n\u001b[32m    367\u001b[39m     logger.warning(\n\u001b[32m    368\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe channel dimension is ambiguous. Got image shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Assuming channels are the first dimension.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Unsupported number of image dimensions: 2"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"tomo_00e463/slice_0235.jpg\")\n",
    "test_img=image_processor(images=img, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ddefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed102edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = model(**test_img)\n",
    "#model.bbox_predictor(**test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98dee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = torch.tensor([img.size[::-1]])\n",
    "\n",
    "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[\n",
    "\n",
    "    0\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "\n",
    "grayscale_image = Image.open(\"tomo_00e463/slice_0235.jpg\").convert(\"L\")\n",
    "\n",
    "# Convert to RGB\n",
    "rgb_image = Image.merge(\"RGB\", (grayscale_image, grayscale_image, grayscale_image))\n",
    "\n",
    "# Save the RGB image\n",
    "rgb_image.save(\"rgb_test_img.jpg\")\n",
    "img = Image.open(\"rgb_test_img.jpg\")\n",
    "\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-base')\n",
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-base')\n",
    "\n",
    "inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "bboxes = outputs.pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b100a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db94aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = torch.tensor([[img.size[1], img.size[0]]])\n",
    "results = image_processor.post_process_object_detection(outputs, threshold=0.005, target_sizes=target_sizes)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22515039",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35521de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "\n",
    "    print(\n",
    "\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa7cec",
   "metadata": {},
   "source": [
    "---\n",
    "## Fine Tune/ Transfer Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a183dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Voxel spacing\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"Motor axis 0\"]!=-1][\"Motor axis 0\"].nsmallest(5))\n",
    "print(df[df[\"Motor axis 1\"]!=-1][\"Motor axis 1\"].nsmallest(5))\n",
    "print(df[df[\"Motor axis 2\"]!=-1][\"Motor axis 2\"].nsmallest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Array shape (axis 0)\"].unique())\n",
    "print(df[\"Array shape (axis 1)\"].unique())\n",
    "print(df[\"Array shape (axis 2)\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93158731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(df[df[\"Array shape (axis 1)\"]!=959]))\n",
    "print(len(df[df[\"Array shape (axis 1)\"]!=959]))\n",
    "print(len(df[df[\"Array shape (axis 2)\"]!=928]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df[\"Motor axis 0\"]!=-1]))\n",
    "print(len(df[df[\"Motor axis 1\"]!=-1]))\n",
    "print(len(df[df[\"Motor axis 2\"]!=-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df[\"Motor axis 0\"]!=-1][\"tomo_id\"].unique()))\n",
    "print(len(df[df[\"Motor axis 1\"]!=-1][\"tomo_id\"].unique()))\n",
    "print(len(df[df[\"Motor axis 2\"]!=-1][\"tomo_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Motor axis 0\"]==466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793526c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Motor axis 2\"]!=-1][\"Motor axis 2\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2adb14b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['tomo_003acc', -1.0, -1.0, -1.0, 500],\n",
       "       ['tomo_00e047', 169.0, 546.0, 603.0, 300],\n",
       "       ['tomo_00e463', 235.0, 403.0, 137.0, 500],\n",
       "       ...,\n",
       "       ['tomo_fea6e8', -1.0, -1.0, -1.0, 300],\n",
       "       ['tomo_ff505c', 111.0, 816.0, 678.0, 300],\n",
       "       ['tomo_ff7c20', -1.0, -1.0, -1.0, 800]],\n",
       "      shape=(737, 5), dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bbox=df.iloc[:,1:6]\n",
    "labels=df_bbox.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cc7413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4912\n",
      "portion of empty = 0.21164282190429207\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "# Create YOLO format label\n",
    "# YOLO format: <class> <x_center> <y_center> <width> <height>\n",
    "# Values are normalized to [0, 1]\n",
    "# {\"file_name\": \"0001.png\", \"objects\": {\"bbox\": [[302.0, 109.0, 73.0, 52.0]], \"categories\": [0]}}\n",
    "BBOXS_SIZE = 70\n",
    "detections = []\n",
    "detections_empty = []\n",
    "trust = 4\n",
    "rng = np.random.default_rng(42)\n",
    "meta_train = []\n",
    "meta_test = []\n",
    "size_empty = 3\n",
    "\n",
    "for a in labels:\n",
    "    if a[1] != -1:\n",
    "        for b in np.arange(int(a[1])-trust, int(a[1])+trust+1):\n",
    "            if b < 0:\n",
    "                continue\n",
    "            width = BBOXS_SIZE\n",
    "            height = BBOXS_SIZE\n",
    "            if BBOXS_SIZE/2 > a[-2]:\n",
    "                width = (a[-2])*2\n",
    "            if BBOXS_SIZE/2 > a[2]:\n",
    "                height = (a[2]-1)*2\n",
    "            detections.append({\"file_name\": f\"{a[0]}/slice_{b:04d}.jpg\", \"objects\": {\n",
    "                              \"bbox\": [[a[-2], a[2], width, height]], \"categories\": [0], \"id\" : [],\"area\":[]}})\n",
    "    else:\n",
    "        random_slice = rng.integers(low=0, high=a[-1], size=size_empty)\n",
    "\n",
    "        for c in random_slice:\n",
    "            detections_empty.append(\n",
    "                {\"file_name\": f\"{a[0]}/slice_{c:04d}.jpg\", \"objects\": {\"bbox\": [[]], \"categories\": []}})\n",
    "\n",
    "print(len(detections_empty+detections))\n",
    "\n",
    "print(f\"portion of empty = {len(detections_empty)/len(detections)}\")\n",
    "\n",
    "# collect train metadata.jason\n",
    "for i in detections:\n",
    "    if i[\"objects\"][\"bbox\"][0][1] >= 0:\n",
    "        source = \"train/\"+i[\"file_name\"]\n",
    "        destination = \"data/train/\"+i[\"file_name\"]\n",
    "\n",
    "        if not os.path.isdir(os.path.dirname(destination)):\n",
    "            os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "        shutil.copyfile(source, destination)\n",
    "        meta_train.append(i)\n",
    "\n",
    "\"\"\"for i in detections_empty:\n",
    "\n",
    "    source = \"train/\"+i[\"file_name\"]\n",
    "    destination = \"data/train/\"+i[\"file_name\"]\n",
    "\n",
    "    if not os.path.isdir(os.path.dirname(destination)):\n",
    "        os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "    shutil.copyfile(source, destination)\n",
    "    meta_train.append(i)\"\"\"\n",
    "\n",
    "\n",
    "# Train test split\n",
    "split = 0.1\n",
    "size_det = len(detections)\n",
    "size_empt = len(detections_empty)\n",
    "\n",
    "test_split_det = int(size_det*split)\n",
    "test_split_empt = int(size_empt*split)\n",
    "\n",
    "indices_test_det = rng.integers(low=0, high=size_det, size=test_split_det)\n",
    "indices_test_empt = rng.integers(low=0, high=size_empt, size=test_split_empt)\n",
    "\n",
    "\n",
    "# copy test files\n",
    "\n",
    "\n",
    "for i in indices_test_det:\n",
    "    if detections[i][\"objects\"][\"bbox\"][0][1] >= 0:\n",
    "        source = \"train/\"+detections[i][\"file_name\"]\n",
    "        destination = \"data/test/\"+detections[i][\"file_name\"]\n",
    "\n",
    "        if not os.path.isdir(os.path.dirname(destination)):\n",
    "            os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "        shutil.copyfile(source, destination)\n",
    "        meta_test.append(detections[i])\n",
    "\n",
    "\n",
    "\"\"\"for i in indices_test_empt:\n",
    "\n",
    "    source = \"train/\"+detections_empty[i][\"file_name\"]\n",
    "    destination = \"data/test/\"+detections_empty[i][\"file_name\"]\n",
    "\n",
    "    if not os.path.isdir(os.path.dirname(destination)):\n",
    "        os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "    shutil.copyfile(source, destination)\n",
    "    meta_test.append(detections_empty[i])\"\"\"\n",
    "\n",
    "\n",
    "# write to jason for hugginface\n",
    "\n",
    "\n",
    "\n",
    "with open(\"data/train/metadata.jsonl\", 'w') as jsonl_output:\n",
    "    for entry in meta_train:\n",
    "        json.dump(entry, jsonl_output)\n",
    "        jsonl_output.write('\\n')\n",
    "        \n",
    "\n",
    "with open(\"data/test/metadata.jsonl\", 'w') as jsonl_output:\n",
    "    for entry in meta_test:\n",
    "        json.dump(entry, jsonl_output)\n",
    "        jsonl_output.write('\\n')\n",
    "\n",
    "\"\"\"\n",
    "with open(\"data/train/metadata.json\", \"w\") as outfile:\n",
    "    json.dump(meta_train, outfile)\n",
    "\n",
    "\n",
    "with open(\"data/test/metadata.json\", \"w\") as outfile:\n",
    "    json.dump(meta_test, outfile)\"\"\"\n",
    "\n",
    "\n",
    "# Create YAML configuration file for YOLO\n",
    "\"\"\"yaml_content = {\n",
    "    'path': yolo_dataset_dir,\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {0: 'motor'}\n",
    "}\"\"\"\n",
    "print(len(indices_test_det)+len(indices_test_empt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec3c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset=[]\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"data/\")\n",
    "\n",
    "#id2label = {0: \"motor\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c0dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in dataset:\n",
    "\n",
    "    split = dataset[\"train\"].train_test_split(0.075, seed=1337)\n",
    "\n",
    "    dataset[\"train\"] = split[\"train\"]\n",
    "\n",
    "    dataset[\"validation\"] = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5407ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'objects'],\n",
       "        num_rows: 3749\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'objects'],\n",
       "        num_rows: 405\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'objects'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e37816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6da027",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[\"train\"][2][\"image\"]\n",
    "\n",
    "annotations = dataset[\"train\"][2][\"objects\"]\n",
    "\n",
    "annotations[\"bbox\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812db21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[\"train\"][2][\"image\"]\n",
    "\n",
    "annotations = dataset[\"train\"][2][\"objects\"]\n",
    "\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "box = annotations[\"bbox\"][0]\n",
    "\n",
    "x, y, w, h = tuple(box)\n",
    "\n",
    "\n",
    "draw.rectangle((x-w/2, y-h/2, x + w/2, y + h/2), outline=\"white\", width=1)\n",
    "\n",
    "draw.text((x-w/2, y-h/2), \"Motor\", fill=\"black\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ca56b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "MAX_SIZE= 720 \n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "\n",
    "    \"hustvl/yolos-base\",\n",
    "\n",
    "    do_resize=True,\n",
    "\n",
    "    size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE},\n",
    "\n",
    "    do_pad=True,\n",
    "\n",
    "    pad_size={\"height\": MAX_SIZE, \"width\": MAX_SIZE},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5c4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_augment_and_transform = A.Compose(\n",
    "\n",
    "    [\n",
    "\n",
    "        A.Perspective(p=0.1),\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "\n",
    "        A.HueSaturationValue(p=0.1),\n",
    "\n",
    "    ],\n",
    "\n",
    "    bbox_params=A.BboxParams(format=\"coco\", clip=True, min_area=25),\n",
    "\n",
    ")\n",
    "\n",
    "validation_transform = A.Compose(\n",
    "\n",
    "    [A.NoOp()],\n",
    "\n",
    "    bbox_params=A.BboxParams(format=\"coco\", clip=True),\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e54dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_image_annotations_as_coco(image_id, bboxes):\n",
    "\n",
    "    \"\"\"Format one set of image annotations to the COCO format\n",
    "\n",
    "    Args:\n",
    "\n",
    "        image_id (str): image id. e.g. \"0001\"\n",
    "\n",
    "        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n",
    "\n",
    "        areas (List[float]): list of corresponding areas to provided bounding boxes\n",
    "\n",
    "        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n",
    "\n",
    "            ([center_x, center_y, width, height] in absolute coordinates)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict: {\n",
    "\n",
    "            \"image_id\": image id,\n",
    "\n",
    "            \"annotations\": list of formatted annotations\n",
    "\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    for bbox in bboxes:\n",
    "\n",
    "        formatted_annotation = {\n",
    "\n",
    "            \"image_id\": image_id,\n",
    "\n",
    "            \"category_id\": 0,\n",
    "\n",
    "            \"iscrowd\": 0,\n",
    "\n",
    "            \"area\": 0,\n",
    "\n",
    "            \"bbox\": list(bbox),\n",
    "\n",
    "        }\n",
    "\n",
    "        annotations.append(formatted_annotation)\n",
    "\n",
    "    return {\n",
    "\n",
    "        \"image_id\": image_id,\n",
    "\n",
    "        \"annotations\": annotations,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92769af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][2][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6087d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n",
    "\n",
    "    \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n",
    "\n",
    "    images = []\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        # apply augmentations\n",
    "\n",
    "        output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"categories\"])\n",
    "\n",
    "        images.append(output[\"image\"])\n",
    "\n",
    "        # format annotations in COCO format\n",
    "\n",
    "        formatted_annotations = format_image_annotations_as_coco(\n",
    "\n",
    "            image, output[\"bboxes\"]\n",
    "\n",
    "        )\n",
    "\n",
    "        annotations.append(formatted_annotations)\n",
    "\n",
    "    # Apply the image processor transformations: resizing, rescaling, normalization\n",
    "\n",
    "    result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "\n",
    "    if not return_pixel_mask:\n",
    "\n",
    "        result.pop(\"pixel_mask\", None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa80106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-1.6727, -1.4329, -1.7240,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.7583, -1.7412, -1.1418,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-2.0323, -1.7069, -1.0733,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-1.6898, -1.5014, -1.2103,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.6727, -1.4843, -1.2103,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.6555, -1.4843, -1.2103,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.5805, -1.3354, -1.6331,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.6681, -1.6506, -1.0378,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.9482, -1.6155, -0.9678,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-1.5980, -1.4055, -1.1078,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.5805, -1.3880, -1.1078,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.5630, -1.3880, -1.1078,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.3513, -1.1073, -1.4036,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.4384, -1.4210, -0.8110,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.7173, -1.3861, -0.7413,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-1.3687, -1.1770, -0.8807,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.3513, -1.1596, -0.8807,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.3339, -1.1596, -0.8807,  ...,  0.0000,  0.0000,  0.0000]]]),\n",
       " 'labels': {'size': tensor([720, 720]), 'image_id': tensor([[[[37, 37, 37],\n",
       "           [75, 75, 75],\n",
       "           [66, 66, 66],\n",
       "           ...,\n",
       "           [41, 41, 41],\n",
       "           [31, 31, 31],\n",
       "           [ 0,  0,  0]],\n",
       " \n",
       "          [[71, 71, 71],\n",
       "           [60, 60, 60],\n",
       "           [63, 63, 63],\n",
       "           ...,\n",
       "           [36, 36, 36],\n",
       "           [27, 27, 27],\n",
       "           [ 0,  0,  0]],\n",
       " \n",
       "          [[31, 31, 31],\n",
       "           [14, 14, 14],\n",
       "           [46, 46, 46],\n",
       "           ...,\n",
       "           [31, 31, 31],\n",
       "           [23, 23, 23],\n",
       "           [ 0,  0,  0]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[52, 52, 52],\n",
       "           [58, 58, 58],\n",
       "           [71, 71, 71],\n",
       "           ...,\n",
       "           [67, 67, 67],\n",
       "           [59, 59, 59],\n",
       "           [83, 83, 83]],\n",
       " \n",
       "          [[53, 53, 53],\n",
       "           [58, 58, 58],\n",
       "           [71, 71, 71],\n",
       "           ...,\n",
       "           [77, 77, 77],\n",
       "           [47, 47, 47],\n",
       "           [32, 32, 32]],\n",
       " \n",
       "          [[54, 54, 54],\n",
       "           [59, 59, 59],\n",
       "           [71, 71, 71],\n",
       "           ...,\n",
       "           [77, 77, 77],\n",
       "           [67, 67, 67],\n",
       "           [32, 32, 32]]]]), 'class_labels': tensor([0]), 'boxes': tensor([[0.5490, 0.5052, 0.0729, 0.0729]]), 'area': tensor([0.]), 'iscrowd': tensor([0]), 'orig_size': tensor([960, 928])}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform_batch = partial(\n",
    "\n",
    "    augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n",
    "\n",
    ")\n",
    "\n",
    "validation_transform_batch = partial(\n",
    "\n",
    "    augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n",
    "\n",
    ")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_batch)\n",
    "\n",
    "dataset[\"validation\"] = dataset[\"validation\"].with_transform(validation_transform_batch)\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].with_transform(validation_transform_batch)\n",
    "\n",
    "dataset[\"train\"][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c58d4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n",
    "\n",
    "    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        boxes (torch.Tensor): Bounding boxes in YOLO format\n",
    "\n",
    "        image_size (Tuple[int, int]): Image size in format (height, width)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # convert center to corners format\n",
    "\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    # convert to absolute coordinates\n",
    "\n",
    "    height, width = image_size\n",
    "\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c19aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "id2label = {0: \"Motor\"}\n",
    "label2id = {\"motor\":0}\n",
    "@dataclass\n",
    "\n",
    "class ModelOutput:\n",
    "\n",
    "    logits: torch.Tensor\n",
    "\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "\n",
    "def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Compute mean average mAP, mAR and their variants for the object detection task.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
    "\n",
    "        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n",
    "\n",
    "        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "    # For metric computation we need to provide:\n",
    "\n",
    "    #  - targets in a form of list of dictionaries with keys \"boxes\", \"labels\"\n",
    "\n",
    "    #  - predictions in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n",
    "\n",
    "    image_sizes = []\n",
    "\n",
    "    post_processed_targets = []\n",
    "\n",
    "    post_processed_predictions = []\n",
    "\n",
    "    # Collect targets in the required format for metric computation\n",
    "\n",
    "    for batch in targets:\n",
    "\n",
    "        # collect image sizes, we will need them for predictions post processing\n",
    "\n",
    "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
    "\n",
    "        image_sizes.append(batch_image_sizes)\n",
    "\n",
    "        # collect targets in the required format for metric computation\n",
    "\n",
    "        # boxes were converted to YOLO format needed for model training\n",
    "\n",
    "        # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "\n",
    "        for image_target in batch:\n",
    "\n",
    "            boxes = torch.tensor(image_target[\"boxes\"])\n",
    "\n",
    "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
    "\n",
    "            labels = torch.tensor(image_target[\"class_labels\"])\n",
    "\n",
    "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "    # Collect predictions in the required format for metric computation,\n",
    "\n",
    "    # model produce boxes in YOLO format, then image_processor convert them to Pascal VOC format\n",
    "\n",
    "    for batch, target_sizes in zip(predictions, image_sizes):\n",
    "\n",
    "        batch_logits, batch_boxes = batch[1], batch[2]\n",
    "\n",
    "        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "\n",
    "        post_processed_output = image_processor.post_process_object_detection(\n",
    "\n",
    "            output, threshold=threshold, target_sizes=target_sizes\n",
    "\n",
    "        )\n",
    "\n",
    "        post_processed_predictions.extend(post_processed_output)\n",
    "\n",
    "    # Compute metrics\n",
    "\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "\n",
    "    metric.update(post_processed_predictions, post_processed_targets)\n",
    "\n",
    "    metrics = metric.compute()\n",
    "\n",
    "    # Replace list of per class metrics with separate metric for each class\n",
    "\n",
    "    classes = metrics.pop(\"classes\")\n",
    "\n",
    "    map_per_class = metrics.pop(\"map_per_class\")\n",
    "\n",
    "    mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "\n",
    "    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "\n",
    "        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "\n",
    "        metrics[f\"map_{class_name}\"] = class_map\n",
    "\n",
    "        metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "eval_compute_metrics_fn = partial(\n",
    "\n",
    "    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27f82256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de7490b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"yolos_base_finetuned_yolololos\",\n",
    "\n",
    "    num_train_epochs=30,\n",
    "\n",
    "    fp16=False,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    weight_decay=1e-4,\n",
    "\n",
    "    max_grad_norm=0.01,\n",
    "\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "\n",
    "    greater_is_better=True,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    save_total_limit=2,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    eval_do_concat_batches=False,\n",
    "\n",
    "    push_to_hub=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1301ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of YolosForObjectDetection were not initialized from the model checkpoint at hustvl/yolos-base and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.layers.2.weight: found shape torch.Size([92, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- class_labels_classifier.layers.2.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "\n",
    "    \"hustvl/yolos-base\",\n",
    "\n",
    "    id2label=id2label,\n",
    "\n",
    "    label2id=label2id,\n",
    "\n",
    "    ignore_mismatched_sizes=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=dataset[\"train\"],\n",
    "\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "\n",
    "    processing_class=image_processor,\n",
    "\n",
    "    data_collator=collate_fn,\n",
    "\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4607770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tensor([[928, 928],\n",
    "        [959, 928],\n",
    "        [928, 960],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 928]])\n",
    "tensor([[928, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928]])\n",
    "tensor([[959, 928],\n",
    "        [928, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[924, 956],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [928, 960],\n",
    "        [959, 928],\n",
    "        [928, 928]])\n",
    "tensor([[924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [959, 928],\n",
    "        [960, 928]])\n",
    "tensor([[960, 928],\n",
    "        [960, 928],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [928, 960],\n",
    "        [960, 928]])\n",
    "tensor([[959, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 928]])\n",
    "tensor([[959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 960]])\n",
    "tensor([[928, 960],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[959, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [959, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [928, 960]])\n",
    "tensor([[960, 928],\n",
    "        [928, 928],\n",
    "        [959, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [928, 960]])\n",
    "tensor([[924, 956],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [959, 928],\n",
    "        [960, 928]])\n",
    "tensor([[960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 960],\n",
    "        [960, 928],\n",
    "        [924, 956]])\n",
    "tensor([[928, 960],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [959, 928],\n",
    "        [924, 956],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [924, 956]])\n",
    "tensor([[959, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[924, 956],\n",
    "        [928, 960],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 928],\n",
    "        [928, 928],\n",
    "        [960, 928]])\n",
    "tensor([[959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [928, 928],\n",
    "        [928, 928],\n",
    "        [924, 956],\n",
    "        [959, 928]])\n",
    "tensor([[960, 928],\n",
    "        [928, 928],\n",
    "        [928, 960],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[959, 928],\n",
    "        [924, 956],\n",
    "        [959, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [924, 956]])\n",
    "tensor([[960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[924, 956],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [928, 960]])\n",
    "tensor([[928, 960],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [928, 960]])\n",
    "tensor([[960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 960],\n",
    "        [959, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[960, 928],\n",
    "        [928, 960],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 960],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[960, 928],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 960],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[959, 928],\n",
    "        [959, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [924, 956]])\n",
    "tensor([[960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [959, 928],\n",
    "        [960, 928]])\n",
    "tensor([[924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[928, 960],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [928, 928],\n",
    "        [928, 928],\n",
    "        [928, 960]])\n",
    "tensor([[924, 956],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [960, 928]])\n",
    "tensor([[960, 928],\n",
    "        [928, 960],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [928, 960],\n",
    "        [928, 960],\n",
    "        [960, 928],\n",
    "        [924, 956]])\n",
    "tensor([[960, 928],\n",
    "        [959, 928],\n",
    "        [928, 928],\n",
    "        [959, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [928, 928]])\n",
    "tensor([[924, 956],\n",
    "        [928, 960],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [928, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928]])\n",
    "tensor([[928, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [959, 928],\n",
    "        [928, 960]])\n",
    "tensor([[959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [924, 956],\n",
    "        [928, 960],\n",
    "        [924, 956],\n",
    "        [928, 928]])\n",
    "tensor([[924, 956],\n",
    "        [960, 928],\n",
    "        [924, 956],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [928, 960]])\n",
    "tensor([[960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [959, 928],\n",
    "        [960, 928],\n",
    "        [960, 928],\n",
    "        [928, 928]])\n",
    "tensor([[924]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
