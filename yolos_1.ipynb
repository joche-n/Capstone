{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced30fe8",
   "metadata": {},
   "source": [
    "xxx yolos\n",
    "\n",
    "what do we need for yolo?\n",
    "get all the labeld images?\n",
    "\n",
    "transfrome labels?\n",
    "\n",
    "weights?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a55852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection,AutoImageProcessor\n",
    "from PIL import Image, ImageDraw\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-base\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"tomo_00e463/slice_0235.jpg\")\n",
    "test_img=image_processor(images=img, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ddefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed102edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = model(**test_img)\n",
    "#model.bbox_predictor(**test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98dee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = torch.tensor([img.size[::-1]])\n",
    "\n",
    "results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[\n",
    "\n",
    "    0\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "\n",
    "grayscale_image = Image.open(\"tomo_00e463/slice_0235.jpg\").convert(\"L\")\n",
    "\n",
    "# Convert to RGB\n",
    "rgb_image = Image.merge(\"RGB\", (grayscale_image, grayscale_image, grayscale_image))\n",
    "\n",
    "# Save the RGB image\n",
    "rgb_image.save(\"rgb_test_img.jpg\")\n",
    "img = Image.open(\"rgb_test_img.jpg\")\n",
    "\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-base')\n",
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-base')\n",
    "\n",
    "inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "bboxes = outputs.pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b100a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db94aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = torch.tensor([[img.size[1], img.size[0]]])\n",
    "results = image_processor.post_process_object_detection(outputs, threshold=0.005, target_sizes=target_sizes)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22515039",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35521de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "\n",
    "    print(\n",
    "\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa7cec",
   "metadata": {},
   "source": [
    "---\n",
    "## Fine Tune/ Transfer Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a183dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Voxel spacing\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"Motor axis 0\"]!=-1][\"Motor axis 0\"].nsmallest(5))\n",
    "print(df[df[\"Motor axis 1\"]!=-1][\"Motor axis 1\"].nsmallest(5))\n",
    "print(df[df[\"Motor axis 2\"]!=-1][\"Motor axis 2\"].nsmallest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Array shape (axis 0)\"].unique())\n",
    "print(df[\"Array shape (axis 1)\"].unique())\n",
    "print(df[\"Array shape (axis 2)\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93158731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(df[df[\"Array shape (axis 1)\"]!=959]))\n",
    "print(len(df[df[\"Array shape (axis 1)\"]!=959]))\n",
    "print(len(df[df[\"Array shape (axis 2)\"]!=928]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df[\"Motor axis 0\"]!=-1]))\n",
    "print(len(df[df[\"Motor axis 1\"]!=-1]))\n",
    "print(len(df[df[\"Motor axis 2\"]!=-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df[\"Motor axis 0\"]!=-1][\"tomo_id\"].unique()))\n",
    "print(len(df[df[\"Motor axis 1\"]!=-1][\"tomo_id\"].unique()))\n",
    "print(len(df[df[\"Motor axis 2\"]!=-1][\"tomo_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Motor axis 0\"]==466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793526c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Motor axis 2\"]!=-1][\"Motor axis 2\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbox=df.iloc[:,1:6]\n",
    "labels=df_bbox.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YOLO format label\n",
    "# YOLO format: <class> <x_center> <y_center> <width> <height>\n",
    "# Values are normalized to [0, 1]\n",
    "# {\"file_name\": \"0001.png\", \"objects\": {\"bbox\": [[302.0, 109.0, 73.0, 52.0]], \"categories\": [0]}}\n",
    "BBOXS_SIZE = 70\n",
    "detections = []\n",
    "detections_empty = []\n",
    "trust = 4\n",
    "rng = np.random.default_rng(42)\n",
    "meta_train = []\n",
    "meta_test = []\n",
    "size_empty = 3\n",
    "\n",
    "for a in labels:\n",
    "    if a[1] != -1:\n",
    "        for b in np.arange(int(a[1])-trust, int(a[1])+trust+1):\n",
    "            if b < 0:\n",
    "                continue\n",
    "            width = BBOXS_SIZE\n",
    "            height = BBOXS_SIZE\n",
    "            if BBOXS_SIZE/2 > a[-2]:\n",
    "                width = (a[-2])*2\n",
    "            if BBOXS_SIZE/2 > a[2]:\n",
    "                height = (a[2]-1)*2\n",
    "            detections.append({\"file_name\": f\"{a[0]}/slice_{b:04d}.jpg\", \"objects\": {\n",
    "                              \"bbox\": [[a[-2], a[2], width, height]], \"categories\": [0], \"id\" : [],\"area\":[]}})\n",
    "    else:\n",
    "        random_slice = rng.integers(low=0, high=a[-1], size=size_empty)\n",
    "\n",
    "        for c in random_slice:\n",
    "            detections_empty.append(\n",
    "                {\"file_name\": f\"{a[0]}/slice_{c:04d}.jpg\", \"objects\": {\"bbox\": [[]], \"categories\": []}})\n",
    "\n",
    "print(len(detections_empty+detections))\n",
    "\n",
    "print(f\"portion of empty = {len(detections_empty)/len(detections)}\")\n",
    "\n",
    "# collect train metadata.jason\n",
    "for i in detections:\n",
    "    if i[\"objects\"][\"bbox\"][0][1] >= 0:\n",
    "        source = \"train/\"+i[\"file_name\"]\n",
    "        destination = \"data/train/\"+i[\"file_name\"]\n",
    "\n",
    "        if not os.path.isdir(os.path.dirname(destination)):\n",
    "            os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "        shutil.copyfile(source, destination)\n",
    "        meta_train.append(i)\n",
    "\n",
    "for i in detections_empty:\n",
    "\n",
    "    source = \"train/\"+i[\"file_name\"]\n",
    "    destination = \"data/train/\"+i[\"file_name\"]\n",
    "\n",
    "    if not os.path.isdir(os.path.dirname(destination)):\n",
    "        os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "    shutil.copyfile(source, destination)\n",
    "    meta_train.append(i)\n",
    "\n",
    "\n",
    "# Train test split\n",
    "split = 0.1\n",
    "size_det = len(detections)\n",
    "size_empt = len(detections_empty)\n",
    "\n",
    "test_split_det = int(size_det*split)\n",
    "test_split_empt = int(size_empt*split)\n",
    "\n",
    "indices_test_det = rng.integers(low=0, high=size_det, size=test_split_det)\n",
    "indices_test_empt = rng.integers(low=0, high=size_empt, size=test_split_empt)\n",
    "\n",
    "\n",
    "# copy test files\n",
    "\n",
    "\n",
    "for i in indices_test_det:\n",
    "    if detections[i][\"objects\"][\"bbox\"][0][1] >= 0:\n",
    "        source = \"train/\"+detections[i][\"file_name\"]\n",
    "        destination = \"data/test/\"+detections[i][\"file_name\"]\n",
    "\n",
    "        if not os.path.isdir(os.path.dirname(destination)):\n",
    "            os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "        shutil.copyfile(source, destination)\n",
    "        meta_test.append(detections[i])\n",
    "\n",
    "\n",
    "for i in indices_test_empt:\n",
    "\n",
    "    source = \"train/\"+detections_empty[i][\"file_name\"]\n",
    "    destination = \"data/test/\"+detections_empty[i][\"file_name\"]\n",
    "\n",
    "    if not os.path.isdir(os.path.dirname(destination)):\n",
    "        os.mkdir(os.path.dirname(destination))\n",
    "\n",
    "    shutil.copyfile(source, destination)\n",
    "    meta_test.append(detections_empty[i])\n",
    "\n",
    "\n",
    "# write to jason for hugginface\n",
    "\n",
    "\n",
    "\n",
    "with open(\"data/train/metadata.jsonl\", 'w') as jsonl_output:\n",
    "    for entry in meta_train:\n",
    "        json.dump(entry, jsonl_output)\n",
    "        jsonl_output.write('\\n')\n",
    "        \n",
    "\n",
    "with open(\"data/test/metadata.jsonl\", 'w') as jsonl_output:\n",
    "    for entry in meta_test:\n",
    "        json.dump(entry, jsonl_output)\n",
    "        jsonl_output.write('\\n')\n",
    "\n",
    "\"\"\"\n",
    "with open(\"data/train/metadata.json\", \"w\") as outfile:\n",
    "    json.dump(meta_train, outfile)\n",
    "\n",
    "\n",
    "with open(\"data/test/metadata.json\", \"w\") as outfile:\n",
    "    json.dump(meta_test, outfile)\"\"\"\n",
    "\n",
    "\n",
    "# Create YAML configuration file for YOLO\n",
    "\"\"\"yaml_content = {\n",
    "    'path': yolo_dataset_dir,\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {0: 'motor'}\n",
    "}\"\"\"\n",
    "print(len(indices_test_det)+len(indices_test_empt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset=[]\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"data/\")\n",
    "\n",
    "#id2label = {0: \"motor\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in dataset:\n",
    "\n",
    "    split = dataset[\"train\"].train_test_split(0.075, seed=1337)\n",
    "\n",
    "    dataset[\"train\"] = split[\"train\"]\n",
    "\n",
    "    dataset[\"validation\"] = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5407ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][\"image\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e37816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6da027",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[\"train\"][2][\"image\"]\n",
    "\n",
    "annotations = dataset[\"train\"][2][\"objects\"]\n",
    "\n",
    "annotations[\"bbox\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812db21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[\"train\"][2][\"image\"]\n",
    "\n",
    "annotations = dataset[\"train\"][2][\"objects\"]\n",
    "\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "box = annotations[\"bbox\"][0]\n",
    "\n",
    "x, y, w, h = tuple(box)\n",
    "\n",
    "\n",
    "draw.rectangle((x-w/2, y-h/2, x + w/2, y + h/2), outline=\"white\", width=1)\n",
    "\n",
    "draw.text((x-w/2, y-h/2), \"Motor\", fill=\"black\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca56b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SIZE= 924 \n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "\n",
    "    \"hustvl/yolos-base\",\n",
    "\n",
    "    do_resize=True,\n",
    "\n",
    "    size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE},\n",
    "\n",
    "    do_pad=True,\n",
    "\n",
    "    pad_size={\"height\": MAX_SIZE, \"width\": MAX_SIZE},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_augment_and_transform = A.Compose(\n",
    "\n",
    "    [\n",
    "\n",
    "        A.Perspective(p=0.1),\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "\n",
    "        A.HueSaturationValue(p=0.1),\n",
    "\n",
    "    ],\n",
    "\n",
    "    bbox_params=A.BboxParams(format=\"coco\", clip=True, min_area=25),\n",
    "\n",
    ")\n",
    "\n",
    "validation_transform = A.Compose(\n",
    "\n",
    "    [A.NoOp()],\n",
    "\n",
    "    bbox_params=A.BboxParams(format=\"coco\", clip=True),\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e54dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_image_annotations_as_coco(image_id, bboxes):\n",
    "\n",
    "    \"\"\"Format one set of image annotations to the COCO format\n",
    "\n",
    "    Args:\n",
    "\n",
    "        image_id (str): image id. e.g. \"0001\"\n",
    "\n",
    "        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n",
    "\n",
    "        areas (List[float]): list of corresponding areas to provided bounding boxes\n",
    "\n",
    "        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n",
    "\n",
    "            ([center_x, center_y, width, height] in absolute coordinates)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict: {\n",
    "\n",
    "            \"image_id\": image id,\n",
    "\n",
    "            \"annotations\": list of formatted annotations\n",
    "\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    for bbox in bboxes:\n",
    "\n",
    "        formatted_annotation = {\n",
    "\n",
    "            \"image_id\": image_id,\n",
    "\n",
    "            \"category_id\": 0,\n",
    "\n",
    "            \"iscrowd\": 0,\n",
    "\n",
    "            \"area\": 0\n",
    "\n",
    "            \"bbox\": list(bbox),\n",
    "\n",
    "        }\n",
    "\n",
    "        annotations.append(formatted_annotation)\n",
    "\n",
    "    return {\n",
    "\n",
    "        \"image_id\": image_id,\n",
    "\n",
    "        \"annotations\": annotations,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92769af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][2][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augment_and_transform(dataset[\"train\"][2][\"image\"],dataset[\"train\"][2][\"objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6087d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n",
    "\n",
    "    \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n",
    "\n",
    "    images = []\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        # apply augmentations\n",
    "\n",
    "        output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        images.append(output[\"image\"])\n",
    "\n",
    "        # format annotations in COCO format\n",
    "\n",
    "        formatted_annotations = format_image_annotations_as_coco(\n",
    "\n",
    "            output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n",
    "\n",
    "        )\n",
    "\n",
    "        annotations.append(formatted_annotations)\n",
    "\n",
    "    # Apply the image processor transformations: resizing, rescaling, normalization\n",
    "\n",
    "    result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "\n",
    "    if not return_pixel_mask:\n",
    "\n",
    "        result.pop(\"pixel_mask\", None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa80106",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_batch = partial(\n",
    "\n",
    "    augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n",
    "\n",
    ")\n",
    "\n",
    "validation_transform_batch = partial(\n",
    "\n",
    "    augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n",
    "\n",
    ")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_batch)\n",
    "\n",
    "dataset[\"validation\"] = dataset[\"validation\"].with_transform(validation_transform_batch)\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].with_transform(validation_transform_batch)\n",
    "\n",
    "dataset[\"train\"][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n",
    "\n",
    "    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        boxes (torch.Tensor): Bounding boxes in YOLO format\n",
    "\n",
    "        image_size (Tuple[int, int]): Image size in format (height, width)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # convert center to corners format\n",
    "\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    # convert to absolute coordinates\n",
    "\n",
    "    height, width = image_size\n",
    "\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "id2label = {0: \"Motor\"}\n",
    "label2id = {\"motor\":0}\n",
    "@dataclass\n",
    "\n",
    "class ModelOutput:\n",
    "\n",
    "    logits: torch.Tensor\n",
    "\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "\n",
    "def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Compute mean average mAP, mAR and their variants for the object detection task.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
    "\n",
    "        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n",
    "\n",
    "        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "    # For metric computation we need to provide:\n",
    "\n",
    "    #  - targets in a form of list of dictionaries with keys \"boxes\", \"labels\"\n",
    "\n",
    "    #  - predictions in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n",
    "\n",
    "    image_sizes = []\n",
    "\n",
    "    post_processed_targets = []\n",
    "\n",
    "    post_processed_predictions = []\n",
    "\n",
    "    # Collect targets in the required format for metric computation\n",
    "\n",
    "    for batch in targets:\n",
    "\n",
    "        # collect image sizes, we will need them for predictions post processing\n",
    "\n",
    "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
    "\n",
    "        image_sizes.append(batch_image_sizes)\n",
    "\n",
    "        # collect targets in the required format for metric computation\n",
    "\n",
    "        # boxes were converted to YOLO format needed for model training\n",
    "\n",
    "        # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "\n",
    "        for image_target in batch:\n",
    "\n",
    "            boxes = torch.tensor(image_target[\"boxes\"])\n",
    "\n",
    "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
    "\n",
    "            labels = torch.tensor(image_target[\"class_labels\"])\n",
    "\n",
    "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "    # Collect predictions in the required format for metric computation,\n",
    "\n",
    "    # model produce boxes in YOLO format, then image_processor convert them to Pascal VOC format\n",
    "\n",
    "    for batch, target_sizes in zip(predictions, image_sizes):\n",
    "\n",
    "        batch_logits, batch_boxes = batch[1], batch[2]\n",
    "\n",
    "        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "\n",
    "        post_processed_output = image_processor.post_process_object_detection(\n",
    "\n",
    "            output, threshold=threshold, target_sizes=target_sizes\n",
    "\n",
    "        )\n",
    "\n",
    "        post_processed_predictions.extend(post_processed_output)\n",
    "\n",
    "    # Compute metrics\n",
    "\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "\n",
    "    metric.update(post_processed_predictions, post_processed_targets)\n",
    "\n",
    "    metrics = metric.compute()\n",
    "\n",
    "    # Replace list of per class metrics with separate metric for each class\n",
    "\n",
    "    classes = metrics.pop(\"classes\")\n",
    "\n",
    "    map_per_class = metrics.pop(\"map_per_class\")\n",
    "\n",
    "    mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "\n",
    "    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "\n",
    "        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "\n",
    "        metrics[f\"map_{class_name}\"] = class_map\n",
    "\n",
    "        metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "eval_compute_metrics_fn = partial(\n",
    "\n",
    "    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f82256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7490b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"detr_finetuned_cppe5\",\n",
    "\n",
    "    num_train_epochs=30,\n",
    "\n",
    "    fp16=False,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    weight_decay=1e-4,\n",
    "\n",
    "    max_grad_norm=0.01,\n",
    "\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "\n",
    "    greater_is_better=True,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    save_total_limit=2,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    eval_do_concat_batches=False,\n",
    "\n",
    "    push_to_hub=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1301ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "\n",
    "    \"hustvl/yolos-base\",\n",
    "\n",
    "    id2label=id2label,\n",
    "\n",
    "    label2id=label2id,\n",
    "\n",
    "    ignore_mismatched_sizes=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=dataset[\"train\"],\n",
    "\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "\n",
    "    processing_class=image_processor,\n",
    "\n",
    "    data_collator=collate_fn,\n",
    "\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
